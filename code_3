import numpy as np
import os
from PIL import Image
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from scipy.linalg import eigh
import matplotlib.pyplot as plt
import zipfile
import tempfile
import shutil

# ---------  Data Preprocessing ---------
class DataPreprocessor:
    def __init__(self, dataset_path='orl_faces'):
        self.dataset_path = dataset_path
        self.image_size = (92, 112)
        self.num_subjects = 40
        self.images_per_subject = 10
        self.total_images = self.num_subjects * self.images_per_subject
        self.vector_length = self.image_size[0] * self.image_size[1]
        
    def load_dataset(self):
        data_matrix = np.zeros((self.total_images, self.vector_length))
        label_vector = np.zeros(self.total_images)
        img_counter = 0

        for subject_id in range(1, self.num_subjects + 1):
            subject_dir = os.path.join(self.dataset_path, f's{subject_id}')
            if not os.path.exists(subject_dir):
                raise FileNotFoundError(f"Directory not found: {subject_dir}")
            for img_num in range(1, self.images_per_subject + 1):
                img_path = os.path.join(subject_dir, f'{img_num}.pgm')
                if not os.path.exists(img_path):
                    raise FileNotFoundError(f"Image file not found: {img_path}")
                try:
                    with Image.open(img_path) as img:
                        img_vector = np.array(img).flatten()
                        data_matrix[img_counter] = img_vector
                        label_vector[img_counter] = subject_id
                    img_counter += 1
                except Exception as e:
                    print(f"Error loading image {img_path}: {str(e)}")
        return data_matrix[:img_counter], label_vector[:img_counter]
    
    def split_data(self, data_matrix, label_vector):
        X_train = data_matrix[::2]
        X_test = data_matrix[1::2]
        y_train = label_vector[::2]
        y_test = label_vector[1::2]
        return X_train, X_test, y_train, y_test
    
    def visualize_samples(self, data_matrix, label_vector, num_samples=5):
        plt.figure(figsize=(15, 3))
        for i in range(num_samples):
            idx = np.random.randint(0, len(data_matrix))
            img = data_matrix[idx].reshape(self.image_size)
            plt.subplot(1, num_samples, i+1)
            plt.imshow(img, cmap='gray')
            plt.title(f'Subject {int(label_vector[idx])}')
            plt.axis('off')
        plt.tight_layout()
        plt.show()
    
    def run_pipeline(self):
        print("Loading dataset...")
        D, y = self.load_dataset()
        print("\nDisplaying sample images:")
        self.visualize_samples(D, y)
        print("\nSplitting dataset...")
        X_train, X_test, y_train, y_test = self.split_data(D, y)
        print("\nDataset information:")
        print(f"- Full data matrix shape: {D.shape}")
        print(f"- Training set shape: {X_train.shape}")
        print(f"- Test set shape: {X_test.shape}")
        print(f"- Number of classes: {len(np.unique(y))}")
        return X_train, X_test, y_train, y_test

# ---------  PCA, LDA & KNN ---------
class FaceRecognizer:
    def __init__(self, X_train, X_test, y_train, y_test):
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test
        self.pca_results = {}
        self.lda_results = None

    def pca(self, alpha=0.95):
        mu = np.mean(self.X_train, axis=0)
        Z = self.X_train - mu
        cov = np.cov(Z, rowvar=False)
        eigenvalues, eigenvectors = eigh(cov)
        idx = np.argsort(eigenvalues)[::-1]
        eigenvalues = eigenvalues[idx]
        eigenvectors = eigenvectors[:, idx]
        total_variance = np.sum(eigenvalues)
        explained_variance = np.cumsum(eigenvalues) / total_variance
        r = np.argmax(explained_variance >= alpha) + 1
        Ur = eigenvectors[:, :r]
        X_train_pca = (self.X_train - mu) @ Ur
        X_test_pca = (self.X_test - mu) @ Ur
        return X_train_pca, X_test_pca, Ur, mu

    def evaluate_pca(self, alphas=[0.80, 0.85, 0.90, 0.95]):
        results = {}
        for alpha in alphas:
            X_train_pca, X_test_pca, _, _ = self.pca(alpha)
            knn = KNeighborsClassifier(n_neighbors=1)
            knn.fit(X_train_pca, self.y_train)
            y_pred = knn.predict(X_test_pca)
            accuracy = accuracy_score(self.y_test, y_pred)
            results[alpha] = {
                'accuracy': accuracy,
                'n_components': X_train_pca.shape[1]
            }
        self.pca_results = results
        return results

    def lda(self):
        n_samples, n_features = self.X_train.shape
        n_classes = len(np.unique(self.y_train))
        mean_total = np.mean(self.X_train, axis=0)
        Sw = np.zeros((n_features, n_features))
        Sb = np.zeros((n_features, n_features))
        for c in np.unique(self.y_train):
            X_c = self.X_train[self.y_train == c]
            mean_c = np.mean(X_c, axis=0)
            n_c = X_c.shape[0]
            Sw += (X_c - mean_c).T @ (X_c - mean_c)
            mean_diff = (mean_c - mean_total).reshape(-1, 1)
            Sb += n_c * (mean_diff @ mean_diff.T)
        Sw += np.eye(Sw.shape[0]) * 1e-6
        eigenvalues, eigenvectors = eigh(Sb, Sw)
        idx = np.argsort(eigenvalues)[::-1]
        W = eigenvectors[:, idx][:, :n_classes-1].real
        X_train_lda = self.X_train @ W
        X_test_lda = self.X_test @ W
        return X_train_lda, X_test_lda, W

    def evaluate_lda(self):
        X_train_lda, X_test_lda, _ = self.lda()
        knn = KNeighborsClassifier(n_neighbors=1)
        knn.fit(X_train_lda, self.y_train)
        y_pred = knn.predict(X_test_lda)
        accuracy = accuracy_score(self.y_test, y_pred)
        self.lda_results = accuracy
        return accuracy

    def knn_tuning(self, k_values=[1, 3, 5, 7], alpha=0.95):
        print("\nRunning KNN Tuning for PCA and LDA...")
        X_train_pca, X_test_pca, _, _ = self.pca(alpha)
        X_train_lda, X_test_lda, _ = self.lda()
        pca_accuracies = []
        lda_accuracies = []

        for k in k_values:
            knn_pca = KNeighborsClassifier(n_neighbors=k)
            knn_pca.fit(X_train_pca, self.y_train)
            y_pred_pca = knn_pca.predict(X_test_pca)
            pca_accuracies.append(accuracy_score(self.y_test, y_pred_pca))

            knn_lda = KNeighborsClassifier(n_neighbors=k)
            knn_lda.fit(X_train_lda, self.y_train)
            y_pred_lda = knn_lda.predict(X_test_lda)
            lda_accuracies.append(accuracy_score(self.y_test, y_pred_lda))

        # Plot results
        plt.figure(figsize=(10, 5))
        plt.plot(k_values, pca_accuracies, marker='o', label='PCA')
        plt.plot(k_values, lda_accuracies, marker='s', label='LDA')
        plt.title('KNN Accuracy vs K')
        plt.xlabel('K')
        plt.ylabel('Accuracy')
        plt.xticks(k_values)
        plt.ylim(0, 1.05)
        plt.grid(True)
        plt.legend()
        plt.tight_layout()
        plt.show()

        # Print table
        print("\nAccuracy Table:")
        print(f"{'K':>3} | {'PCA Accuracy':>13} | {'LDA Accuracy':>13}")
        print("-" * 36)
        for i, k in enumerate(k_values):
            print(f"{k:>3} | {pca_accuracies[i]:>13.2%} | {lda_accuracies[i]:>13.2%}")

    def visualize_results(self):
        plt.figure(figsize=(15, 5))
        plt.subplot(1, 2, 1)
        alphas = list(self.pca_results.keys())
        accuracies = [self.pca_results[a]['accuracy'] for a in alphas]
        n_components = [self.pca_results[a]['n_components'] for a in alphas]
        plt.plot(alphas, accuracies, 'bo-')
        plt.title('PCA Accuracy vs Variance Retention')
        plt.xlabel('Alpha')
        plt.ylabel('Accuracy')
        for i, txt in enumerate(n_components):
            plt.annotate(f'n={txt}', (alphas[i], accuracies[i]))
        plt.subplot(1, 2, 2)
        plt.bar(['LDA'], [self.lda_results])
        plt.title('LDA Accuracy')
        plt.ylabel('Accuracy')
        plt.ylim(0, 1.1)
        plt.tight_layout()
        plt.show()

# --------- Dataset Extraction Helpers ---------
def find_dataset_root(extracted_path):
    for root, dirs, files in os.walk(extracted_path):
        if any(f's{i}' in dirs for i in range(1, 41)):
            return root
    return None

def extract_dataset(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    dataset_root = find_dataset_root(extract_to)
    if dataset_root is None:
        if os.path.exists(os.path.join(extract_to, 's1')):
            return extract_to
        raise FileNotFoundError("Dataset folders not found.")
    return dataset_root

# --------- Main Execution ---------
if __name__ == "__main__":
    zip_path = r"C:/Users/dell/Downloads/archive (1).zip"  # ‚Üê Update with your zip path
    extract_to = tempfile.mkdtemp()

    try:
        print(f"Extracting dataset from {zip_path}...")
        dataset_path = extract_dataset(zip_path, extract_to)
        print(f"Found dataset at: {dataset_path}")

        preprocessor = DataPreprocessor(dataset_path=dataset_path)
        X_train, X_test, y_train, y_test = preprocessor.run_pipeline()

        recognizer = FaceRecognizer(X_train, X_test, y_train, y_test)

        print("\nEvaluating PCA with different variance retention levels...")
        pca_results = recognizer.evaluate_pca()

        print("\nEvaluating LDA...")
        lda_accuracy = recognizer.evaluate_lda()

        recognizer.visualize_results()

        recognizer.knn_tuning()

    except Exception as e:
        print(f"\nError: {str(e)}")
    finally:
        shutil.rmtree(extract_to, ignore_errors=True)
